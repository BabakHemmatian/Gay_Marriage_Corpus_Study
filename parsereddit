#!/usr/bin/python3
from __future__ import print_function
from json import JSONDecoder    # imports the code to parse json
import bz2                      # imports the code to open bz2-compressed files
import os
from sys import argv            # import to read script arguments
import string
import HTMLParser
import codecs
import string
import re
# import gensim
# from gensim import corpora
import nltk
nltk.download('wordnet')
# from nltk.corpus import stopwords
import string
stop = set(nltk.corpus.stopwords.words('english'))
exclude = set(string.punctuation)
lemma = nltk.stem.WordNetLemmatizer()
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized
# This is the json parser
decoder = JSONDecoder(encoding='utf-8')
path = "D:/Reddit_Comments"
for filename in os.listdir(path):
    if filename.endswith(".bz2"):
# open the file as a text file (rt="read text"), in utf8 encoding
        fin = bz2.BZ2File(filename,'r')
        fout = open('Print.txt', 'a+')#bz2.BZ2File(argv[2],'w')
    # every line is a comment
        print(filename,sep="/n",file=fout)
        for jsonline in fin:
        #
        # parse the json, and turn it to regular text
        #
            comment = decoder.decode(jsonline)
            body = HTMLParser.HTMLParser().unescape(comment["body"])
            body = clean(body)
            body = body.replace('\ndeleted\n','')
            body = body.encode('utf-8')
        # get rid of line breaks, and print
        #
            print(" ".join(body.split()), sep="/t", file=fout)
fout.close()
    #     continue
    # else:
    #     continue
