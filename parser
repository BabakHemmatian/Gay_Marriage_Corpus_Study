#!/python27
# -*- coding: utf-8 -*-
# import the required modules and functions
from __future__ import print_function
import json
from utils import *
import os
import tarfile
import datetime
import bz2
import string
import HTMLParser
import re
import sys
import nltk
nltk.download('wordnet')
nltk.download('stopwords')
from collections import OrderedDict
# set default file encoding
reload(sys)
sys.setdefaultencoding('utf8')
### choose database and outlets (Reddit = Reddit, Fb = Facebook)
database = "Fb"
liberal = ["nytimes","cnn","HuffPost","BuzzFeed","ABCNews"]
centrist = ["businessinsider","TheHill","wsj","forbes"]
conservative = ["FoxNews","Breitbart","NYPost","DailyMail","DailyCaller"]
outlet = []
# comment out the kind of outlet you don't want to include:
for x in liberal:
    outlet.append(x)
for x in centrist:
    outlet.append(x)
for x in conservative:
    outlet.append(x)
### Preprocessing
## determine the set of stopwords used in preprocessing. Retest without removing a subset
stop = set(nltk.corpus.stopwords.words('english'))
keepers = ["how","should","can","will","just","so","few","more","most","why","how","all","any","about","against","because","as","have","has","had","is","are","was","were","be","been","being","they","them","their","theirs","themselves","i","me","my","myself","we","our","ours","ourselves","you","your","yours","yourself","yourselves"]
stop = [word.encode('ascii') for word in stop if word not in keepers]
# since the s in "was" didn't encode properly during preprocessing, if the word is to be removed, "wa" should be added to the list of stopwords by removing the hashtag from the next line:
# stop.append("wa")
exclude = set(string.punctuation)
lemma = nltk.stem.WordNetLemmatizer()
## define the preprocessing function to remove punctuation, special characters and stopwords, as well as normalize
def clean(text):
    text = text.replace("'"," ")
    special_free = ""
    for words in text.lower().split():
        words = re.sub('[^A-Za-z0-9]+', ' ', words)
        special_free = special_free+" "+words
    stop_free = " ".join([i for i in special_free.split() if i not in stop])
    no_punc = re.compile('|'.join(map(re.escape, exclude)))
    punc_free = no_punc.sub(' ',stop_free)
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())
    return normalized
### unpack data
# json parser
decoder = json.JSONDecoder(encoding='utf-8')
# where the data is
file_path = os.path.abspath(sys.argv[0])
path = os.path.dirname(file_path)
# define the relevance filter
GAYMAR = getFilterRegex()
# create dictionaries for storing the number of documents by media, year and outlet
RC_year = dict()
Fb_year = dict()
### parse data
for filename in os.listdir(path):
    ## for the Reddit database
    if database == "Reddit":
        if os.path.splitext(filename)[1] == '.bz2' and 'RC' in filename:
    # open the file as a text file (rt="read text"), in utf8 encoding
            fin = bz2.BZ2File(filename,'r')
            output_filename = "%s_prep" % (os.path.splitext(filename)[0])
            fout = open(output_filename,'a+')
        # every line is a comment
            # parse the json, and turn it to regular text
            for line in fin:
                comment = decoder.decode(line)
                body = HTMLParser.HTMLParser().unescape(comment["body"])
                # filter comments by relevance to the topic
                if len(GAYMAR.findall(body)) > 0:
                    # record the number of documents by year
                    created_at = datetime.datetime.fromtimestamp(int(comment["created_utc"])).strftime('%Y')
                    if created_at not in RC_year:
                        RC_year[created_at] = 1
                    else:
                        RC_year[created_at] += 1
                    # preprocess and print comments
                    body = clean(body)
                    body = body.replace("\n","")
                    body = body.replace("'"," ")
                    body = body.encode("utf-8")
                    print(" ".join(body.split()), sep="\t",end="\n", file=fout)
            fout.close()
    ## for the Facebook database
    elif database == "Fb":
        # unzip data files
        if os.path.splitext(filename)[1] == '.bz2' and any(x in filename for x in outlet):
            current_outlet = [x for x in outlet if x in filename]
            current_outlet = current_outlet[0]
            pin = tarfile.open(path+'/'+filename, "r:bz2")
            pin.extractall()
            new_path = path + "/" + os.path.splitext(filename)[0].replace(".tar","")
            pin2 = tarfile.open(new_path, "r:")
            pin2.extractall()
            new_path = new_path + "/" + "fbdata" + "/" + os.path.splitext(filename)[0].replace(".tar","")
            # open and read data files
            for post in os.listdir(new_path):
                if '._' not in post:
                    output_filename = "%s_prep" % (os.path.splitext(post)[0])
                    fout = open(new_path+"/"+output_filename,'a+')
                    with open(new_path+'/'+post) as f:
                        data = f.readline()
                        jsondata = json.loads(data)
                        for jsonline in jsondata:
                            # clean and record the comments
                            body = HTMLParser.HTMLParser().unescape(jsonline["message"])
                            body = clean(body)
                            body = body.replace("\n","")
                            body = body.encode('utf8')
                            print(" ".join(body.split()), end='\n', file=fout)
                            # record the number of documents by year and outlet
                            created_at = str(jsonline["created_time"][0:4])
                            if created_at+current_outlet not in Fb_year:
                                Fb_year[created_at+current_outlet] = 1
                            else:
                                Fb_year[created_at+current_outlet] += 1
                            # record comments that are replies to other comments
                            if "children" in jsonline:
                                for responses in jsonline["children"]:
                                    # clean and record the comments
                                    response = HTMLParser.HTMLParser().unescape(responses["message"])
                                    response = clean(response)
                                    response = response.encode('utf8')
                                    print(" ".join(response.split()), sep = "\t",end='\n',file=fout)
                                    # record the number of reply documents by year and outlet
                                    created_at = str(responses["created_time"][0:4])
                                    if created_at+current_outlet not in Fb_year:
                                        Fb_year[created_at+current_outlet] = 1
                                    else:
                                        Fb_year[created_at+current_outlet] += 1
                            # print("\n",end="",file=fout) # needed if replies are considered part of the same document as the original comment. The ending of the comments would need to be changed to a "space" too in that case
### show the distribution of documents by media, specific outlet and year
fcount = open(database+"_count",'a+')
if database == "Reddit":
    print(OrderedDict(sorted(RC_year.items())),end='\n',file=fcount)
else:
    print(OrderedDict(sorted(Fb_year.items())),end='\n',file=fcount)
### determine what percentage of the posts in each year was relevant
## Reddit
# the following line has the number of relevant comments in the Reddit database as of 9/26/17:
# relevant = {'2006': 152, '2007': 636, '2008': 3246, '2009': 6141, '2010': 11543, '2011': 28401, '2012': 84897, '2013': 91411, '2014': 76372, '2015': 124377, '2016': 116611, '2017': 69214}
d = {}
with open("RC_Count_total.txt") as f:
    for line in f:
       (key, val) = line.split()
       d[key] = int(val)
year = {}
perc_rel = {}
for keys in d:
    if str(keys[3:7]) in year:
        year[str(keys[3:7])] += d[keys]
    else:
        year[str(keys[3:7])] = d[keys]
for key in relevant:
    perc_rel[key] = float(relevant[key]) / float(year[key])
print OrderedDict(sorted(perc_rel.items()))
## Facebook
